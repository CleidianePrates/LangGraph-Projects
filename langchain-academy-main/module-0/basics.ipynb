{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660ce795-9307-4c2c-98a1-beabcb36c740",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-0/basics.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/56295530-getting-set-up-video-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
   "metadata": {},
   "source": [
    "# Academia LangChain\n",
    "\n",
    "Bem-vindo à Academia LangChain!\n",
    "\n",
    "## Contexto\n",
    "\n",
    "Na LangChain, nosso objetivo é facilitar a construção de aplicações LLM. Um tipo de aplicação LLM que você pode construir é um agente. Há muito entusiasmo em torno da construção de agentes porque eles podem automatizar uma ampla gama de tarefas que antes eram impossíveis.\n",
    "\n",
    "Na prática, porém, é incrivelmente difícil construir sistemas que executem essas tarefas de forma confiável. À medida que trabalhamos com nossos usuários para colocar agentes em produção, aprendemos que um maior controle é frequentemente necessário. Você pode precisar que um agente sempre chame uma ferramenta específica primeiro ou use prompts diferentes com base em seu estado.\n",
    "\n",
    "Para resolver esse problema, construímos o [LangGraph](https://langchain-ai.github.io/langgraph/) — um framework para construir aplicações de agente e multi-agente. Separado do pacote LangChain, a filosofia central de design do LangGraph é ajudar os desenvolvedores a adicionar melhor precisão e controle nos fluxos de trabalho dos agentes, adequados para a complexidade dos sistemas do mundo real.\n",
    "\n",
    "## Estrutura do Curso\n",
    "\n",
    "O curso está estruturado como um conjunto de módulos, com cada módulo focado em um tema específico relacionado ao LangGraph. Você verá uma pasta para cada módulo, que contém uma série de notebooks. Um vídeo acompanhará cada notebook para ajudar a percorrer os conceitos, mas os notebooks também são independentes, o que significa que contêm explicações e podem ser visualizados independentemente dos vídeos. Cada pasta de módulo também contém uma pasta `studio`, que contém um conjunto de gráficos que podem ser carregados no [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio), nosso IDE para construir aplicações LangGraph.\n",
    "\n",
    "## Configuração\n",
    "\n",
    "Antes de começar, siga as instruções no `README` para criar um ambiente e instalar as dependências.\n",
    "\n",
    "## Modelos de chat\n",
    "\n",
    "Neste curso, usaremos [Modelos de Chat](https://python.langchain.com/v0.2/docs/concepts/#chat-models), que fazem algumas coisas como receber uma sequência de mensagens como entradas e retornar mensagens de chat como saídas. LangChain não hospeda nenhum Modelo de Chat, em vez disso, dependemos de integrações de terceiros. [Aqui](https://python.langchain.com/v0.2/docs/integrations/chat/) está uma lista de integrações de modelos de chat de terceiros dentro do LangChain! Por padrão, o curso usará [ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/) porque é tanto popular quanto eficiente. Como observado, certifique-se de que você tem uma `OPENAI_API_KEY`.\n",
    "\n",
    "Vamos verificar se sua `OPENAI_API_KEY` está configurada e, se não estiver, você será solicitado a inseri-la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9a52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a15227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f35b",
   "metadata": {},
   "source": [
    "[Aqui](https://python.langchain.com/v0.2/docs/how_to/#chat-models) está um guia útil para todas as coisas que você pode fazer com modelos de chat, mas mostraremos alguns destaques abaixo. Se você executou `pip install -r requirements.txt` conforme indicado no README, então você instalou o pacote `langchain-openai`. Com isso, podemos instanciar nosso objeto de modelo `ChatOpenAI`. Se você está se inscrevendo na API pela primeira vez, deve receber [créditos gratuitos](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517) que podem ser aplicados a qualquer um dos modelos. Você pode ver os preços para vários modelos [aqui](https://openai.com/api/pricing/). Os notebooks usarão por padrão o `gpt-4o` porque é um bom equilíbrio entre qualidade, preço e velocidade [veja mais aqui](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini), mas você também pode optar pelos modelos da série `gpt-3.5` de menor preço.\n",
    "\n",
    "Existem [alguns parâmetros padrão](https://python.langchain.com/v0.2/docs/concepts/#chat-models) que podemos definir com modelos de chat. Dois dos mais comuns são:\n",
    "\n",
    "* `model`: o nome do modelo\n",
    "* `temperature`: a temperatura de amostragem\n",
    "\n",
    "A `temperature` controla a aleatoriedade ou criatividade da saída do modelo, onde uma temperatura baixa (próxima a 0) gera saídas mais determinísticas e focadas. Isso é bom para tarefas que requerem precisão ou respostas factuais. Alta temperatura (próxima a 1) é boa para tarefas criativas ou para gerar respostas variadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19a54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450d1b",
   "metadata": {},
   "source": [
    "Os modelos de chat no LangChain possuem vários [métodos padrão](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). Na maior parte do tempo, usaremos:\n",
    "\n",
    "* `stream`: transmitir pedaços da resposta de forma contínua\n",
    "* `invoke`: chamar a cadeia em uma entrada\n",
    "\n",
    "E, como mencionado, os modelos de chat recebem [mensagens](https://python.langchain.com/v0.2/docs/concepts/#messages) como entrada. As mensagens têm um papel (role) que descreve quem está enviando a mensagem e uma propriedade de conteúdo (content). Falaremos muito mais sobre isso mais tarde, mas aqui vamos apenas mostrar o básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1280e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90122d973c', 'id': 'chatcmpl-BWXRqDotK1HP1EMmGLhkT4jnJEF4K', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a6e70ce2-a8de-4e0e-8592-66b5d6d7968c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "gpt4o_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73e4c",
   "metadata": {},
   "source": [
    "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27c6c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f5bdcc3276', 'id': 'chatcmpl-BWXRuKhVpI5azGxRYHJkyp29q14uK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c8aa1c95-8036-4084-94d4-745cda511e9e-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc2f0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BWXRxo97BZgOkv5KnkckDbTIuVT5r', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--76dd3926-407b-4b02-9721-3894b0f0662e-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0e5a",
   "metadata": {},
   "source": [
    "The interface is consistent across all chat models and models are typically initialized once at the start up each notebooks. \n",
    "\n",
    "So, you can easily switch between models without changing the downstream code if you have strong preference for another provider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0069a",
   "metadata": {},
   "source": [
    "## Search Tools\n",
    "\n",
    "You'll also see [Tavily](https://tavily.com/) in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. Some lessons (in Module 4) will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091dff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d69da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d06f87e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'LangGraph: The Future of Advanced Multi-Agent Workflows - Medium',\n",
       "  'url': 'https://medium.com/@piyushkashyap045/langgraph-the-future-of-advanced-multi-agent-workflows-2449df6fdf3a',\n",
       "  'content': '1. What is LangGraph?\\nLangGraph is a framework built on top of LangChain that allows developers to create complex workflows using graph-based models. A graph in this context consists of nodes (representing functions or tools) and edges (representing connections between those nodes). This structure enables LangGraph to handle complex AI-driven applications with ease.\\nKey Definition:\\n\\nLangGraph: A framework within LangChain for creating workflows using Directed Cyclic Graphs (DCGs). [...] In\\nDev Genius\\nby\\nNeural pAi\\n🚀 The Ultimate Guide to LangGraph: All Aspects Explained --------------------------------------------------------- ### LangGraph is an innovative framework designed to create, manage, and execute graph-based workflows powered by large language models (LLMs)…\\nFeb 23\\n45',\n",
       "  'score': 0.96306777},\n",
       " {'title': 'What is LangGraph? - Analytics Vidhya',\n",
       "  'url': 'https://www.analyticsvidhya.com/blog/2024/07/langgraph-revolutionizing-ai-agent/',\n",
       "  'content': 'To sum up, LangGraph is a major advancement in the development of AI agents. It enables developers to push the limits of what’s possible with AI agents by eliminating the shortcomings of earlier systems and offering a flexible, graph-based framework for agent construction and execution. LangGraph is positioned to influence the direction of artificial intelligence significantly in the future. [...] Frameworks such as LangGraph are becoming increasingly important as AI develops. LangGraph is making the next generation of AI applications possible by offering a versatile and strong framework for developing and overseeing AI agents. [...] In this way, we can add different kinds of tools to the LLM so that we can get our queries answered even if LLM alone can’t answer. Thus LLM agents will be far more useful in many scanarios.\\n\\nWhat LangGraph Offers?\\n\\nLangGraph offers a powerful toolset for building complex AI systems. It provides a framework for creating agentic systems that can reason, make decisions, and interact with multiple data sources. Key features include:\\n\\nReal-World Example of LangGraph',\n",
       "  'score': 0.947386},\n",
       " {'title': 'What Is LangGraph and How to Use It? - DataCamp',\n",
       "  'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'content': 'LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for developing agent runtimes. With LangGraph, we can easily build robust, scalable, and flexible multi-agent systems. [...] Home\\nTutorials\\n\\nLangGraph Tutorial: What Is LangGraph and How to Use It?\\nLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\\nContents\\nJun 26, 2024 \\xa0· 12 min read\\nContents\\n\\nWhat Is LangGraph?\\nGraph structure\\nState management\\n\\nCoordination\\n\\n\\nWhy LangGraph?\\n\\nSimplified development\\nFlexibility\\nScalability\\n\\nFault tolerance\\n\\n\\nGetting Started With LangGraph [...] If you want to learn more about the LangChain ecosystem, I recommend this introduction to LangChain.\\nWhat Is LangGraph?\\nLangGraph enables us to create stateful, multi-actor applications utilizing LLMs as easily as possible. It extends the capabilities of LangChain, introducing the ability to create and manage cyclical graphs, which are pivotal for developing sophisticated agent runtimes. The core concepts of LangGraph include: graph structure, state management, and coordination.\\nGraph structure',\n",
       "  'score': 0.93844646}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
