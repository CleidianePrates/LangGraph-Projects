{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660ce795-9307-4c2c-98a1-beabcb36c740",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-0/basics.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/56295530-getting-set-up-video-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
   "metadata": {},
   "source": [
    "# Academia LangChain\n",
    "\n",
    "Bem-vindo √† Academia LangChain!\n",
    "\n",
    "## Contexto\n",
    "\n",
    "Na LangChain, nosso objetivo √© facilitar a constru√ß√£o de aplica√ß√µes LLM. Um tipo de aplica√ß√£o LLM que voc√™ pode construir √© um agente. H√° muito entusiasmo em torno da constru√ß√£o de agentes porque eles podem automatizar uma ampla gama de tarefas que antes eram imposs√≠veis.\n",
    "\n",
    "Na pr√°tica, por√©m, √© incrivelmente dif√≠cil construir sistemas que executem essas tarefas de forma confi√°vel. √Ä medida que trabalhamos com nossos usu√°rios para colocar agentes em produ√ß√£o, aprendemos que um maior controle √© frequentemente necess√°rio. Voc√™ pode precisar que um agente sempre chame uma ferramenta espec√≠fica primeiro ou use prompts diferentes com base em seu estado.\n",
    "\n",
    "Para resolver esse problema, constru√≠mos o [LangGraph](https://langchain-ai.github.io/langgraph/) ‚Äî um framework para construir aplica√ß√µes de agente e multi-agente. Separado do pacote LangChain, a filosofia central de design do LangGraph √© ajudar os desenvolvedores a adicionar melhor precis√£o e controle nos fluxos de trabalho dos agentes, adequados para a complexidade dos sistemas do mundo real.\n",
    "\n",
    "## Estrutura do Curso\n",
    "\n",
    "O curso est√° estruturado como um conjunto de m√≥dulos, com cada m√≥dulo focado em um tema espec√≠fico relacionado ao LangGraph. Voc√™ ver√° uma pasta para cada m√≥dulo, que cont√©m uma s√©rie de notebooks. Um v√≠deo acompanhar√° cada notebook para ajudar a percorrer os conceitos, mas os notebooks tamb√©m s√£o independentes, o que significa que cont√™m explica√ß√µes e podem ser visualizados independentemente dos v√≠deos. Cada pasta de m√≥dulo tamb√©m cont√©m uma pasta `studio`, que cont√©m um conjunto de gr√°ficos que podem ser carregados no [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio), nosso IDE para construir aplica√ß√µes LangGraph.\n",
    "\n",
    "## Configura√ß√£o\n",
    "\n",
    "Antes de come√ßar, siga as instru√ß√µes no `README` para criar um ambiente e instalar as depend√™ncias.\n",
    "\n",
    "## Modelos de chat\n",
    "\n",
    "Neste curso, usaremos [Modelos de Chat](https://python.langchain.com/v0.2/docs/concepts/#chat-models), que fazem algumas coisas como receber uma sequ√™ncia de mensagens como entradas e retornar mensagens de chat como sa√≠das. LangChain n√£o hospeda nenhum Modelo de Chat, em vez disso, dependemos de integra√ß√µes de terceiros. [Aqui](https://python.langchain.com/v0.2/docs/integrations/chat/) est√° uma lista de integra√ß√µes de modelos de chat de terceiros dentro do LangChain! Por padr√£o, o curso usar√° [ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/) porque √© tanto popular quanto eficiente. Como observado, certifique-se de que voc√™ tem uma `OPENAI_API_KEY`.\n",
    "\n",
    "Vamos verificar se sua `OPENAI_API_KEY` est√° configurada e, se n√£o estiver, voc√™ ser√° solicitado a inseri-la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9a52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a15227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f35b",
   "metadata": {},
   "source": [
    "[Aqui](https://python.langchain.com/v0.2/docs/how_to/#chat-models) est√° um guia √∫til para todas as coisas que voc√™ pode fazer com modelos de chat, mas mostraremos alguns destaques abaixo. Se voc√™ executou `pip install -r requirements.txt` conforme indicado no README, ent√£o voc√™ instalou o pacote `langchain-openai`. Com isso, podemos instanciar nosso objeto de modelo `ChatOpenAI`. Se voc√™ est√° se inscrevendo na API pela primeira vez, deve receber [cr√©ditos gratuitos](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517) que podem ser aplicados a qualquer um dos modelos. Voc√™ pode ver os pre√ßos para v√°rios modelos [aqui](https://openai.com/api/pricing/). Os notebooks usar√£o por padr√£o o `gpt-4o` porque √© um bom equil√≠brio entre qualidade, pre√ßo e velocidade [veja mais aqui](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini), mas voc√™ tamb√©m pode optar pelos modelos da s√©rie `gpt-3.5` de menor pre√ßo.\n",
    "\n",
    "Existem [alguns par√¢metros padr√£o](https://python.langchain.com/v0.2/docs/concepts/#chat-models) que podemos definir com modelos de chat. Dois dos mais comuns s√£o:\n",
    "\n",
    "* `model`: o nome do modelo\n",
    "* `temperature`: a temperatura de amostragem\n",
    "\n",
    "A `temperature` controla a aleatoriedade ou criatividade da sa√≠da do modelo, onde uma temperatura baixa (pr√≥xima a 0) gera sa√≠das mais determin√≠sticas e focadas. Isso √© bom para tarefas que requerem precis√£o ou respostas factuais. Alta temperatura (pr√≥xima a 1) √© boa para tarefas criativas ou para gerar respostas variadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19a54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450d1b",
   "metadata": {},
   "source": [
    "Os modelos de chat no LangChain possuem v√°rios [m√©todos padr√£o](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). Na maior parte do tempo, usaremos:\n",
    "\n",
    "* `stream`: transmitir peda√ßos da resposta de forma cont√≠nua\n",
    "* `invoke`: chamar a cadeia em uma entrada\n",
    "\n",
    "E, como mencionado, os modelos de chat recebem [mensagens](https://python.langchain.com/v0.2/docs/concepts/#messages) como entrada. As mensagens t√™m um papel (role) que descreve quem est√° enviando a mensagem e uma propriedade de conte√∫do (content). Falaremos muito mais sobre isso mais tarde, mas aqui vamos apenas mostrar o b√°sico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1280e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90122d973c', 'id': 'chatcmpl-BWXRqDotK1HP1EMmGLhkT4jnJEF4K', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a6e70ce2-a8de-4e0e-8592-66b5d6d7968c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "gpt4o_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73e4c",
   "metadata": {},
   "source": [
    "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27c6c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f5bdcc3276', 'id': 'chatcmpl-BWXRuKhVpI5azGxRYHJkyp29q14uK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c8aa1c95-8036-4084-94d4-745cda511e9e-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc2f0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BWXRxo97BZgOkv5KnkckDbTIuVT5r', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--76dd3926-407b-4b02-9721-3894b0f0662e-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0e5a",
   "metadata": {},
   "source": [
    "The interface is consistent across all chat models and models are typically initialized once at the start up each notebooks. \n",
    "\n",
    "So, you can easily switch between models without changing the downstream code if you have strong preference for another provider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0069a",
   "metadata": {},
   "source": [
    "## Search Tools\n",
    "\n",
    "You'll also see [Tavily](https://tavily.com/) in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. Some lessons (in Module 4) will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091dff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d69da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d06f87e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'LangGraph: The Future of Advanced Multi-Agent Workflows - Medium',\n",
       "  'url': 'https://medium.com/@piyushkashyap045/langgraph-the-future-of-advanced-multi-agent-workflows-2449df6fdf3a',\n",
       "  'content': '1. What is LangGraph?\\nLangGraph is a framework built on top of LangChain that allows developers to create complex workflows using graph-based models. A graph in this context consists of nodes (representing functions or tools) and edges (representing connections between those nodes). This structure enables LangGraph to handle complex AI-driven applications with ease.\\nKey Definition:\\n\\nLangGraph: A framework within LangChain for creating workflows using Directed Cyclic Graphs (DCGs). [...] In\\nDev Genius\\nby\\nNeural pAi\\nüöÄ The Ultimate Guide to LangGraph: All Aspects Explained --------------------------------------------------------- ### LangGraph is an innovative framework designed to create, manage, and execute graph-based workflows powered by large language models (LLMs)‚Ä¶\\nFeb 23\\n45',\n",
       "  'score': 0.96306777},\n",
       " {'title': 'What is LangGraph? - Analytics Vidhya',\n",
       "  'url': 'https://www.analyticsvidhya.com/blog/2024/07/langgraph-revolutionizing-ai-agent/',\n",
       "  'content': 'To sum up, LangGraph is a major advancement in the development of AI agents. It enables developers to push the limits of what‚Äôs possible with AI agents by eliminating the shortcomings of earlier systems and offering a flexible, graph-based framework for agent construction and execution. LangGraph is positioned to influence the direction of artificial intelligence significantly in the future. [...] Frameworks such as LangGraph are becoming increasingly important as AI develops. LangGraph is making the next generation of AI applications possible by offering a versatile and strong framework for developing and overseeing AI agents. [...] In this way, we can add different kinds of tools to the LLM so that we can get our queries answered even if LLM alone can‚Äôt answer. Thus LLM agents will be far more useful in many scanarios.\\n\\nWhat LangGraph Offers?\\n\\nLangGraph offers a powerful toolset for building complex AI systems. It provides a framework for creating agentic systems that can reason, make decisions, and interact with multiple data sources. Key features include:\\n\\nReal-World Example of LangGraph',\n",
       "  'score': 0.947386},\n",
       " {'title': 'What Is LangGraph and How to Use It? - DataCamp',\n",
       "  'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'content': 'LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for developing agent runtimes. With LangGraph, we can easily build robust, scalable, and flexible multi-agent systems. [...] Home\\nTutorials\\n\\nLangGraph Tutorial: What Is LangGraph and How to Use It?\\nLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\\nContents\\nJun 26, 2024 \\xa0¬∑ 12 min read\\nContents\\n\\nWhat Is LangGraph?\\nGraph structure\\nState management\\n\\nCoordination\\n\\n\\nWhy LangGraph?\\n\\nSimplified development\\nFlexibility\\nScalability\\n\\nFault tolerance\\n\\n\\nGetting Started With LangGraph [...] If you want to learn more about the LangChain ecosystem, I recommend this introduction to LangChain.\\nWhat Is LangGraph?\\nLangGraph enables us to create stateful, multi-actor applications utilizing LLMs as easily as possible. It extends the capabilities of LangChain, introducing the ability to create and manage cyclical graphs, which are pivotal for developing sophisticated agent runtimes. The core concepts of LangGraph include: graph structure, state management, and coordination.\\nGraph structure',\n",
       "  'score': 0.93844646}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
